# kurs-devops

### Решение Задачи  
- Все поставленные задачи решаются через запуск terraform (terraform apply)
- Сруктура проекта терраформ следующая:
  - main.tf - в нем создаются необходимые ВМ для сервисов, расписание snapshots и выходные данные (IP адреса) для ВМ, также задаются локальные значения для создания ВМ (locals) и указываются дополнительные провадеры необходимые для работы (null, time)
  - network.tf - здесь описывается вся сетевая инфраструктура - создается сеть (network-1) и подсети (subnet-1,2,3,4) в соответсвии с заданием
  - security_groups.tf - описываются группы безопасности
  - alb.tf - создается Балансировщик L-7
  - ansible.tf - на основе провайдера null (provisioner "local-exec"), отсюда передаются переменные и запускаются конфигурации Ansible (playbook), подробная работа ansible описана ниже
  - variables.tf - здесь объявляются необходмивые переменные для работы выше описанных ресурсов
  - terraform.tfvars - здесь хранятся чувствительные данные - токены и пароли
  - каталог meta - содержит мета данные для ВМ

- Структура Ansible:
  - В папке ansible хранятся плейбуки для настройки сервисов, каждый плейбук запускается из terraform через провайдера null и передает через параметр --extra-vars необходиммые данные для работы разворачиваемых сервисов (это IP адреса, имена и пароли)
  - Подключение ко всем ВМ происходит через bastion host, как того требудет задание. Для этого в конфигурации Ansible, в инвентори, используется параметр ansible_ssh_common_args с опцией конфигурации SSH - ProxyCommand
  - В самих плейбуках, соответвтвено их наименованиям, описана конфигурация сервисов
  - Также в папке ansible присутствуют еще 2 дирректории templates и files
      - В files расположенны статичные конфигурации которые подгружаются из плейбуков для работы соответствующих сервисов
      - В templates - расположены шаблоны Jinja2 - динамически изменяемые файлы с использованием данных, переданных из Ansible (--extra-vars)

- Основные этапы работы проекта terraform
  - Сначала создаются Сеть, группы безопасности и ВМ
  - Затем создается ALB, расписание snapshots
  - До начала работы Ansible выдерживается пауза 90 секунд для того чтобы ВМ были готовы к разворачиванию плейбуков
  - После паузы активируется ресурсы null провайдера в ansible.tf и запускают плейбуки Ansible
  - Когда плейбуки Ansible завершают свою работу, в проетке terraform выводятся выходные данные и работа завершается
  - Теперь вся инфраструкура доступна и готова к работе. Посмотреть Kibana можно на дефолтном порту 5601 используя имя - elastic и заданный пароль, Grafana порт 3000, имя - admin и соответствующий пароль из настроек работы (см. ниже)

### Настройка для работы:  
- Для работы необходимо указать данные в файле terraform.tfvars - yandex_cloud_token, elastic_passwd и grafana_passwd
- В файле user-data.yaml необходимо указать публичный ключ SSH, а в папку ansible положить соответствующий приватный ключ и указать его имя в ansible.cfg, так же приватный ключ должен быть и в обычном расположении ~/.ssh
- Для входа в grafana пользователь - admin, kibana - elastic
- На всех ВМ пользователь - stas (указывается в meta), в Ansible inventory.ini и ansible.cfg соответвенно тоже указавается пользователь - stas
- Для ВМ Использован образ Ubuntu с id - fd8s3qh62qn5sqoemni6

### Дополнительные настройки:  
- В файле variables.tf можно настроить характеристики ВМ
- Также в variables.tf можно ужесточить политику безопасности, но только после создания всей инфраструктуры, для этого нужно для Переменной управления средой применить default = true и снова запустить terraform apply
- При создании ВМ в файле main.tf можно использовать статические внутренние IP. Для этого нужно добавить ключ для IP в locals и раскомментировать ip_address = each.value.ip в блоке network_interface

### Результаты работы:
Работающая инфраструктура рассходует денежные средства, поэтому представляю только данные о ее работе.  
Если в ходе провери задания потребуется показать работу, то развертка занимает не дольше 10 минут и может быть осуществлена по предварительной договоренности.  

Лог создания ресурсов terraform и настройки ansible представлен в файле: [worklog](results/worklog)  

Скриншоты (католог results): [images](results/)

### Решение дополнительной задачи
https://github.com/Stas-91/kursovai-dop/tree/main  
  
Для решения дополнительной задачи были проведены некоторое изменения проекта terraform и плейбуков Ansible:
- Изменения Сруктуру проекта терраформ:
    - Добавлено instance_group.tf - создание группы ВМ вместо отдельных машин
    - Добавлено postgre.tf - создание кластера PostgreSQL
    - Добавлен Сертикфикат для сайта
    - Изменено network.tf струкутра сети - теперь для каждой ВМ в истнансе и PostgreSQL отдельная подсеть (websub1,2,3), добавлена А запись в ДНС (связывает доменное имя с IP-адресом балансировщика)
    - Изменено security_groups.tf - указаны новые подсети и ресурсы
    - Изменено alb.tf - балансировщик перенастроен на https и группу ВМ в качестве целевой группы
    - Изменено ansible.tf добавлены task для решения дополнительных задач и переменные для них
    - Из каталога meta теперь используются templatefile передавая имя пользователя и публичный SSH ключ через переменную (также где это было возможно имя пользователя тоже передается через переменную var.user_vm)
    - Изменено variables.tf - добавлена переменная для имени пользователя от которого запускаются плейбуки и создаются ВМ variable "user_vm"
    - Изменено terraform.tfvars - добавален пароль для PostgreSQL postgre_passwd, публичный ключ для создания ВМ ssh_public_key

- Измениния в Структуре Ansible:
  - Изменились некоторые плейбуки в соответсии в дополнительным заданием, также соответвенно поменялись и шаблоны и файлы используемые в этих плейбуках.
 
- Основные этапы работы проекта terraform:
  - Основная концепция осталась не изменной, все запускается из проекта terraform (terraform apply), но с одной оговоркой (см ниже - "Что не удалось автомотизировать")
  - После запуска также сначала создаются Сеть, группы безопасности, ВМ, но добавляется еще кластер PostgreSQL.
  - Затем создается instance group - она создается на основе образа ВМ webserv1: webserv1 останавливается, делается snapshot и на его основе разворачивается группа ВМ
  - Далее разворачивается ALB используя instance group как целевую группу
  - Также выдерживается пауза, но больше - 120 сек. Увеличение паузы связано с организацией работы автоматических обновлений безопаности у ВМ Яндекса - иногда плейбуки не могу выполнится из-за блокировки dpkg или apt со стороны автомтического развертывания ВМ.
  - После паузы также активируется ресурсы null провайдера в ansible.tf и запускают плейбуки Ansible и по завершению все также готово к работе.
    
- Отдельным пунктом необходимо рассказать о реализации дополнительных задач со стороны Ansible
  - Всвязи с использование instance group в Prometheus уже нельзя было передать IP адреса хостов созданных ВМ для вебсерверов, т.к. хосты в instance group могут меняться. Для указания хостов в конфигурации Prometheus применен job dynamic_targets, который для работы использует файл targets.json. Создается targets.json с помощью скрипта scan_exporters.sh, который опрашивает нужные подсети и записывает их в targets.json. Запускается scan_exporters.sh с ВМ Prometheus каждые 5 мин через CRON.

- Что не удалось автомотизировать:
  - Я решил получать сертификат SSL для сайта через Яндекс Let's Encrypt используя DNS_TXT чтобы это подходило для ALT, но выдача такого сертификата занимает рандомное время до 24 часов, поэтому сертификат я выпустил из Консоли Управления Yandex Cloud
  - Также я хотел полностью исключить необходимось ручной правки конфигурации, поэтому посторался убрать имя пользователя в переменную, но все же осталась необхомость задавать ее в ansible.cfg и inventory.ini

### Настройка для работы дополнительной задачи:  
- Для работы необходимо указать данные в файле terraform.tfvars - yandex_cloud_token, elastic_passwd, grafana_passwd, postgre_passwd, ssh_public_key
- В папку ansible нужно положить соответствующий приватный ключ SSH и указать его имя в ansible.cfg, так же приватный ключ должен быть и в обычном расположении ~/.ssh
- Для входа в grafana пользователь - admin, kibana - elastic
- На всех ВМ пользователь указывается variables.tf - variable "user_vm" по умолчанию stas, в Ansible inventory.ini и ansible.cfg соответвенно тоже нужно указать пользователь stas или то имя которое задано в variable "user_vm"
- Для ВМ Использован образ Ubuntu с id - fd8s3qh62qn5sqoemni6

### Дополнительные настройки:  
- В файле variables.tf можно настроить характеристики ВМ
- Также variables.tf можно ужесточить политику безопасности, но только после создания всей инфраструктуры, для этого нужно для Переменной управления средой применить default = true и снова запустить terraform apply
- При создании ВМ в файле main.tf можно использовать статические внутренние IP. Для этого нужно добавить ключ для IP в locals и раскоментировать ip_address = each.value.ip в блоке network_interface

### Результаты работы дополнительной задачи:
Работающая инфраструктура из-за кластера PostgreSQL особенно сильно рассходует денежные средства, поэтому представляю только данные о ее работе.  
Если в ходе провери задания потребуется показать работу, то развертка занимает не дольше 10 минут и может быть осуществлена по предварительной договоренности.  

Лог создания ресурсов terraform и настройки ansible представлен в файле: [worklog_dop](results_dop/log_dop)  

Скриншоты (католог results_dop): [images_dop](results_dop/)



